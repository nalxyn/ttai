!pip install -U transformers

#Local Inference on GPU
#Model page: https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct
#‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the model repo and/or on huggingface.js üôè


# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="Qwen/Qwen2.5-Coder-32B-Instruct")
messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe(messages)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-32B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-Coder-32B-Instruct")

#Remote Inference via Inference Providers
#Ensure you have a valid HF_TOKEN set in your environment, running this may bill your account above the free tier. The following Python example shows how to run the model remotely on HF Inference Providers, using the auto provider setting (automatically selects an available inference provider)

rom pprint import pprint
from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="auto",
    api_key='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',
)

completion = client.chat.completions.create(
    model="Qwen/Qwen2.5-Coder-32B-Instruct",
    messages=[
        {
            "role": "user",
            "content": "Write python code for telegram bot. Strictly the code"
        }
    ],
)

print(completion.choices[0].message)
#pprint(completion.choices[0].message['content'])
print(completion.choices[0].message['content'])
